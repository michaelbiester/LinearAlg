{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4605f601-0738-4b7a-87f1-4423fdef540c",
   "metadata": {},
   "source": [
    "# Solving Least Squares Problens iteratively\n",
    "\n",
    "Let $\\mathbf{A}$ denote a $m \\times n$ matrix. The equation \n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "shall be solved. In general the solution is not unique. But the problem can be stated as a least squares problem where the solution vector $\\mathbf{x}$ can be found by minimizing the quadratic norm.\n",
    "\n",
    "At this point the following assumption shall be made.\n",
    "\n",
    "1) matrix $\\mathbf{A}$ is real valued\n",
    "\n",
    "2) vectors $\\mathbf{x}$ and $\\mathbf{b}$ are real valued\n",
    "\n",
    "Thus the goal is to minimize a scalar function $f(\\mathbf{x})$ defined by this equation:\n",
    "\n",
    "$$\\begin{gather}\n",
    "f(\\mathbf{x})  = \\left(\\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{b} \\right)^T \\cdot \\left(\\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{b} \\right) \\\\\n",
    "f(\\mathbf{x})  = \\left(\\mathbf{x}^T \\cdot \\mathbf{A}^T - \\mathbf{b}^T \\right) \\cdot \\left(\\mathbf{A} \\cdot \\mathbf{x} -\\mathbf{b} \\right) \\\\\n",
    "f(\\mathbf{x}) = \\mathbf{x}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} - 2 \\cdot\\mathbf{b}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} +  \\mathbf{b}^T \\cdot \\mathbf{b}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Defining a new function $g(\\mathbf{x}) = \\frac{1}{2} \\cdot f(\\mathbf{x})$ gives us the standard notation of quadratic forms:\n",
    "\n",
    "$$\n",
    "g(\\mathbf{x}) = \\frac{1}{2} \\cdot \\mathbf{x}^T \\cdot \\underbrace{\\mathbf{A}^T \\cdot \\mathbf{A}}_{\\mathbf{U}} \\cdot \\mathbf{x} - \\underbrace{\\mathbf{b}^T \\cdot \\mathbf{A}}_{\\mathbf{q}^T} \\cdot \\mathbf{x} +  \\underbrace{\\frac{1}{2} \\cdot \\mathbf{b}^T \\cdot \\mathbf{b}}_{c}\n",
    "$$\n",
    "\n",
    "Regardless of the shape of matrix $\\mathbf{A}$ the matrix $\\mathbf{U}$ is square, symmetric and positive definite.\n",
    "\n",
    "The gradient $f(\\mathbf{x})$ is computed like this:\n",
    "\n",
    "$$\\begin{gather}\n",
    "f'(\\mathbf{x}) = \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} + \\mathbf{A} \\cdot \\mathbf{A}^T \\cdot \\mathbf{x} - 2 \\cdot \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "f'(\\mathbf{x}) = 2 \\cdot \\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{A}^T \\cdot \\mathbf{b}\\right) \n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Setting the gradient to $0$ results in the *normal* equation for the unknowns vector $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{A}^T \\cdot \\mathbf{b}\n",
    "$$\n",
    "\n",
    "**literature**\n",
    "\n",
    "In her highly readable bachelor thesis\n",
    "\n",
    "`Conjugate Gradients and Conjugate Residuals type methods for solving Least Squares problems from Tomography` (Delft University of Technology)\n",
    "\n",
    "Tamara Kloek discusses various approaches how to solve the normal equation iteratively.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692918e9-472a-401b-bb5a-3b47d41ccb61",
   "metadata": {},
   "source": [
    "Having computed the gradient $f'(\\mathbf{x})$ of the normal equation the direction of *steepest descents* points in the opposite direction $-f'(\\mathbf{x})$.\n",
    "\n",
    "$$\n",
    "-f'(\\mathbf{x}) = -2 \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} + 2 \\cdot \\mathbf{A}^T \\cdot \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Let denote $\\mathbf{x}_{(i)}$ an approximation to the solution vector $\\mathbf{x}$ for the i'th iteration. The direction of steepes descents at that point is then:\n",
    "\n",
    "$$\n",
    "-f'(\\mathbf{x}_{(i)}) = -2 \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}_{(i)} + 2 \\cdot \\mathbf{A}^T \\cdot \\mathbf{b}\n",
    "$$\n",
    "\n",
    "**definitions**\n",
    "\n",
    "Some frequenctly used quantities are defined here.\n",
    "\n",
    "The residual $\\mathbf{r}_{(i)}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_{(i)} = \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x}_{(i)}\n",
    "$$\n",
    "\n",
    "Similarly $\\mathbf{s}_{(i)}$ is defined as\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{s}_{(i)} = \\mathbf{A}^T \\cdot \\mathbf{b} -\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}_{(i)} \\\\\n",
    "\\mathbf{s}_{(i)} = \\mathbf{A}^T \\cdot \\left( \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x}_{(i)} \\right) \\\\\n",
    "\\mathbf{s}_{(i)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(i)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The vector $\\mathbf{s}_{(i)}$ can be seen to be related to the direction of steepest descent by equation :\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{(i)} = - \\frac{1}{2} \\cdot f'(\\mathbf{x}_{(i)})\n",
    "$$\n",
    "\n",
    "Since the exact factor which relates $\\mathbf{s}_{(i)}$ to $-f'(\\mathbf{x}_{(i)})$ is not relevant, we consider $\\mathbf{s}_{(i)}$ as the direction of steepest descent.\n",
    "\n",
    "**optimum stepsize / steepest descent**\n",
    "\n",
    "Going from vector $\\mathbf{x}_{(i)}$ to vector $\\mathbf{x}_{(i+1)}$ in the next iteration uses the direction of steepest descent like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{(i+1)} = \\mathbf{x}_{(i)} + \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)}\n",
    "$$\n",
    "\n",
    "$\\alpha_{(i)}$ denotes the step size which will be determined by minimizing the $f(\\mathbf{x}_{(i+1)}$. Using the chain rule we seek\n",
    "\n",
    "$$\n",
    "\\frac{d}{d \\alpha_{(i)}} f(\\mathbf{x}_{(i+1)} = f'(\\mathbf{x}_{(i+1)}^T \\cdot \\mathbf{s}_{(i)} = -2 \\cdot \\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i)}\n",
    "$$\n",
    "\n",
    "Setting $\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i)}$ to $0$ yields the optimum stepsize paramter:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i)} = 0 \\\\\n",
    "\\left( \\mathbf{A}^T \\cdot \\mathbf{b} -\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}_{(i+1)} \\right)^T \\cdot \\mathbf{s}_{(i)} = 0 \\\\\n",
    "\\left( \\mathbf{A}^T \\cdot \\mathbf{b} -\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot (\\mathbf{x}_{(i)} + \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)}) \\right)^T \\cdot \\mathbf{s}_{(i)} = 0 \\\\\n",
    "\\left( \\mathbf{A}^T \\cdot \\mathbf{b} -\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)}) \\right)^T \\cdot \\mathbf{s}_{(i)} = 0 \\\\\n",
    "\\underbrace{\\left( \\mathbf{A}^T \\cdot \\mathbf{b} - \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}_{(i)} \\right)^T}_{\\mathbf{s}_{(i)}^T} \\cdot \\mathbf{s}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)} = 0 \\\\\n",
    "\\ \\to \\\\\n",
    "\\alpha_{(i)} = \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} }{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)}}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Now we can summarise the steps require to minimize the normal equation using the method of steepest descent\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e54dee-0abf-442f-9e17-544153e4fcaa",
   "metadata": {},
   "source": [
    "## Summary / Method of Steepest Descent\n",
    "\n",
    "We choose a starting value $\\mathbf{x}_{(0)}$.\n",
    "\n",
    "The residual vectors $\\mathbf{r}_{(0)}$ and $\\mathbf{r}_{(0)}$ are computed:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{r}_{(0)} = \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x}_{(0)} \\\\\n",
    "\\mathbf{s}_{(0)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(0)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "For iteration steps $i := [0, 1, \\cdots,\\ N]$ we compute:\n",
    "\n",
    "the step size $\\alpha_{(i)}$:\n",
    "\n",
    "$$\n",
    "\\alpha_{(i)} = \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} }{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)}}\n",
    "$$\n",
    "\n",
    "To calculate the denominator $\\mathbf{s}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)}$ it is sufficient to calculate $\\mathbf{A} \\cdot \\mathbf{s}_{(i)}$ and reuse it as its transpose. To see this we rewrite the denominator:\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)} = \\left(\\mathbf{A} \\cdot \\mathbf{s}_{(i)}\\right)^T \\cdot \\left(\\mathbf{A} \\cdot \\mathbf{s}_{(i)} \\right)\n",
    "$$\n",
    "\n",
    "the next approximation $\\mathbf{x}_{(i+1)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{(i+1)} = \\mathbf{x}_{(i)} + \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)}\n",
    "$$\n",
    "\n",
    "the next residual $\\mathbf{r}_{(i+1)}$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x}_{(i+1)} \\\\\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{b} - \\mathbf{A} \\cdot \\left(\\mathbf{x}_{(i)} + \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)} \\right) \\\\\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{b} -  \\mathbf{A} \\cdot \\mathbf{x}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)} \\\\\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{r}_{(i)}  - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)} \\\\\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "and $\\mathbf{s}_{(i+1)}$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{(i+1)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(i+1)}\n",
    "$$\n",
    "\n",
    "Note that $\\mathbf{A} \\cdot \\mathbf{s}_{(i)}$ is available from the computation of the stepsize $\\alpha_{(i)}$.\n",
    "\n",
    "The square norm of residuals can be monitored to serve as a *termination criterion*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95804a32-1098-4362-bed6-c4c03b2ef582",
   "metadata": {},
   "source": [
    "# Conjugate Gradients Method for the Least Squares problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a4790-caab-4d04-a62c-77159b7b5480",
   "metadata": {},
   "source": [
    "The thesis `Conjugate Gradients and Conjugate Residuals type methods for solving Least Squares problems from Tomography` outlines the procedure of the conjugate gradient method for the least squares problem as follows:\n",
    "\n",
    "**Initialisiation**\n",
    "\n",
    "As for the method of steepest descent we choose a starting value $\\mathbf{x}_{(0)}$.\n",
    "\n",
    "The residual vectors $\\mathbf{r}_{(0)}$ and $\\mathbf{r}_{(0)}$ are computed as has been done for the method of steepest descent:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{r}_{(0)} = \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x}_{(0)} \\\\\n",
    "\\mathbf{s}_{(0)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(0)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "An initial search direction $\\mathbf{p}_{(0)}$ is chosen. It is initially set to the direction of steepest descent. But the search directions will follow a different path in subsequent iterations. \n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{(0)} = \\mathbf{s}_{(0)}\n",
    "$$\n",
    "\n",
    "\n",
    "**Iterations**\n",
    "\n",
    "For iteration steps $i := [0, 1, \\cdots,\\ N]$ we compute:\n",
    "\n",
    "the next approximation $\\mathbf{x}_{(i+1)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{(i+1)} = \\mathbf{x}_{(i)} + \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)}\n",
    "$$\n",
    "\n",
    "the next residual $\\mathbf{r}_{(i+1)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{r}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}\n",
    "$$\n",
    "\n",
    "and the other residual $\\mathbf{s}_{(i+1)}$\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{s}_{(i+1)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(i+1)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \\\\\n",
    "\\mathbf{s}_{(i+1)} = \\mathbf{s}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Finally the computation of a new search direction $\\mathbf{p}_{(i+1)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{(i+1)} = \\mathbf{s}_{(i+1)} + \\beta_{(i)} \\cdot \\mathbf{p}_{(i)}\n",
    "$$\n",
    "\n",
    "**Termination**\n",
    "\n",
    "A suitable termination criterion may use the quadratic norm of either $\\mathbf{r}_{(i+1)}$ or $\\mathbf{s}_{(i+1)}$ and compare it to an acceptance threshold. The iteration is terminated once the criterion is below that threshold.\n",
    "\n",
    "The next step is to determine the yet unknown stepsize parameters $\\alpha_{(i)}$ and $\\beta_{(i)}$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1592d82-e7dc-4d9d-97cc-24868daac8ea",
   "metadata": {},
   "source": [
    "## Computing stepsize parameters $\\alpha_{(i)}$ and $\\beta_{(i)}$\n",
    "\n",
    "\n",
    "Stepsize $\\alpha_{(i)}$ is calculated from the recursion equation \n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{r}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \\\\\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{r}_{(i-1)} - \\alpha_{(i-1)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i-1)} - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Stepsizes $\\alpha_{(i)}$ and $\\alpha_{(i-1)}$ shall be determined such as to minimize $|| \\mathbf{r}_{(i+1)} ||$.\n",
    "\n",
    "$$\n",
    "|| \\mathbf{r}_{(i+1)} || = \\left( \\mathbf{r}_{(i-1)}^T - \\alpha_{(i-1)} \\cdot \\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T - \\alpha_{(i)} \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\right) \\cdot \\left( \\mathbf{r}_{(i-1)} - \\alpha_{(i-1)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i-1)} - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \\right)\n",
    "$$\n",
    "\n",
    "$$\\begin{gather}\n",
    "|| \\mathbf{r}_{(i+1)} || = \\\\\n",
    "\\mathbf{r}_{(i-1)}^T \\cdot \\mathbf{r}_{(i-1)} - \\alpha_{(i-1)} \\cdot \\mathbf{r}_{(i-1)}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i-1)} - \\alpha_{(i)} \\cdot \\mathbf{r}_{(i-1)}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}  \\\\\n",
    "-\\alpha_{(i-1)} \\cdot \\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{r}_{(i-1)} + \\alpha_{(i-1)}^2 \\cdot \\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i-1)} + 2 \\cdot \\alpha_{(i)} \\cdot \\alpha_{(i-1)} \\cdot \\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}  \\\\\n",
    "-\\alpha_{(i)} \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{r}_{(i-1)} + \\alpha_{(i)}^2 \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "\n",
    "For the term $2 \\cdot \\alpha_{(i)} \\cdot \\alpha_{(i-1)} \\cdot \\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} $ we require \n",
    "\n",
    "$$\n",
    "2 \\cdot \\alpha_{(i)} \\cdot \\alpha_{(i-1)} \\cdot \\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} = 0\n",
    "$$\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} = 0\n",
    "$$\n",
    "\n",
    "meaning that search directions $\\mathbf{p}_{(i)}$ and $\\mathbf{p}_{(i-1)}$ are conjugate with respect to matrix $\\mathbf{A}^T \\cdot \\mathbf{A} $.\n",
    "\n",
    "Differention of $|| \\mathbf{r}_{(i+1)} || $ with respect of stepsize $\\alpha_{(i)} $ provides us with:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d \\alpha_{(i)}} || \\mathbf{r}_{(i+1)} || = - 2 \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{r}_{(i-1)} + 2 \\cdot \\alpha_{(i)} \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} = 0 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{(i)} = \\frac{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{r}_{(i-1)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}} = \\frac{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{s}_{(i-1)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}} = \\frac{\\mathbf{s}_{(i-1)}^T \\cdot \\mathbf{p}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}}\n",
    "$$\n",
    "\n",
    "From $\\mathbf{s}_{(i)} = \\mathbf{s}_{(i-1)} - \\alpha_{(i-1)} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i-1)}$ we get:\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{(i-1)} = \\mathbf{s}_{(i)} + \\alpha_{(i-1)} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i-1)}\n",
    "$$\n",
    "\n",
    "Inserting this expression into the equation for $\\alpha_{(i)}$ yields:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\alpha_{(i)} = \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{p}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}} + \\alpha_{(i-1)} \\cdot \\overbrace{\\frac{\\mathbf{p}_{(i-1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\mathbf{p}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}}}^{=0 } \\\\\n",
    "\\alpha_{(i)} = \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{p}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Following the steps in `Conjugate Gradients and Conjugate Residuals type methods for solving Least Squares problems from Tomography` some useful properties must be derived before $\\beta_{(i)}$ can be computed.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d501085-6cc1-420a-a45b-e8778d73f16b",
   "metadata": {},
   "source": [
    "**Proof: $\\mathbf{p}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} = 0$**\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{p}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} = \\mathbf{p}_{(i)}^T \\cdot \\mathbf{s}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \\\\\n",
    "\\mathbf{p}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} = \\mathbf{p}_{(i)}^T \\cdot \\mathbf{s}_{(i)} - \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{p}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}} \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} =  \\mathbf{p}_{(i)}^T \\cdot \\mathbf{s}_{(i)} - \\mathbf{s}_{(i)}^T \\cdot \\mathbf{p}_{(i)} = 0\\\\\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Proof: $\\mathbf{s}_{(i)}^T \\cdot \\mathbf{p}_{(i)} =  \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)}$**\n",
    "\n",
    "Left multiplication of $\\mathbf{p}_{(i+1)} = \\mathbf{s}_{(i+1)} + \\beta_{(i)} \\cdot \\mathbf{p}_{(i)}$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{p}_{(i+1)} = \\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i+1)} + \\beta_{(i)} \\cdot \\underbrace{\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{p}_{(i)}}_{=0} \\\\\n",
    "\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{p}_{(i+1)} = \\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i+1)} \\\\\n",
    "\\ \\to \\\\\n",
    "\\mathbf{s}_{(i)}^T \\cdot \\mathbf{p}_{(i)} = \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "The result of this last proof can be utilized to get a new formula for the stepsize $\\alpha_{(i)} $:\n",
    "\n",
    "$$\n",
    "\\alpha_{(i)} = \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Proof: $\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} = \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)} $**\n",
    "\n",
    "Inserting $\\mathbf{p}_{(i+1)} = \\mathbf{s}_{(i+1)} + \\beta_{(i)} \\cdot \\mathbf{p}_{(i)}$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{p}_{(i+1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i+1)} = \\mathbf{p}_{(i+1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i+1)} + \\beta_{(i)} \\cdot \\underbrace{\\mathbf{p}_{(i+1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}}_{=0} \\\\\n",
    "\\mathbf{p}_{(i+1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i+1)} = \\mathbf{p}_{(i+1)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i+1)} \\\\\n",
    "\\ \\to \\\\\n",
    "\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} = \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{s}_{(i)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Proof:  $\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} = 0$**\n",
    "\n",
    "(Residual is orthogonal to previous / next residual)\n",
    "\n",
    "Inserting the recurrence $\\mathbf{s}_{(i+1)} = \\mathbf{s}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}$\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} = \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \\\\\n",
    "\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} = \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} = \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} - \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}} \\cdot \\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}\\\\\n",
    "\\ \\\\\n",
    "\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} = \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} - \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} = 0\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**computing $\\beta_{(i)}$**\n",
    "\n",
    "With\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{(i+1)} = \\mathbf{s}_{(i+1)} + \\beta_{(i)} \\cdot \\mathbf{p}_{(i)}\n",
    "$$\n",
    "\n",
    "we left multiply by $\\mathbf{p}_{(i)}^T  \\cdot \\mathbf{A}^T \\cdot \\mathbf{A}$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\underbrace{\\mathbf{p}_{(i)}^T  \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i+1)}}_{=0} = \\mathbf{p}_{(i)}^T  \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot  \\mathbf{s}_{(i+1)} + \\beta_{(i)}  \\cdot \\mathbf{p}_{(i)}^T  \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \\\\\n",
    "\\beta_{(i)} = - \\frac{\\mathbf{p}_{(i)}^T  \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot  \\mathbf{s}_{(i+1)}}{\\mathbf{p}_{(i)}  \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "From \n",
    "\n",
    "$$\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{r}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}\n",
    "$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{p}_{(i)} =  \\frac{\\mathbf{r}_{(i)} - \\mathbf{r}_{(i+1)}}{\\alpha_{(i)}}\n",
    "$$\n",
    "\n",
    "Inserting into $\\beta_{(i)}$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\beta_{(i)} = - \\frac{(\\mathbf{r}_{(i)} - \\mathbf{r}_{(i+1)})^T  \\cdot \\mathbf{A} \\cdot  \\mathbf{s}_{(i+1)}}{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} } \\\\\n",
    "\\beta_{(i)} = \\frac{(\\mathbf{s}_{(i+1)} - \\mathbf{s}_{(i)} )^T  \\cdot \\mathbf{s}_{(i+1)}}{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} } \\\\\n",
    "\\beta_{(i)} = \\frac{\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i+1)} - \\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i+1)} }{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} } \\\\\n",
    "\\ \\\\\n",
    "\\beta_{(i)} = \\frac{\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i+1)}}{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} } \n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Having computed the stepsize the method of conjugate gradients for the least squares problem can summarized again.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399a0d5-7e22-467f-a417-e02051c3b26e",
   "metadata": {},
   "source": [
    "**Initialisiation**\n",
    "\n",
    "we choose a starting value $\\mathbf{x}_{(0)}$.\n",
    "\n",
    "The residual vectors $\\mathbf{r}_{(0)}$ and $\\mathbf{r}_{(0)}$ are computed:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{r}_{(0)} = \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x}_{(0)} \\\\\n",
    "\\mathbf{s}_{(0)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(0)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "An initial search direction $\\mathbf{p}_{(0)}$ is chosen. It is initially set to the direction of steepest descent. But the search directions will follow a different path in subsequent iterations. \n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{(0)} = \\mathbf{s}_{(0)}\n",
    "$$\n",
    "\n",
    "\n",
    "**Iterations**\n",
    "\n",
    "For iteration steps $i := [0, 1, \\cdots,\\ N]$ we compute:\n",
    "\n",
    "The step size $\\alpha_{(i)}$:\n",
    "\n",
    "$$\n",
    "\\alpha_{(i)} = \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)}}{\\mathbf{p}_{(i)}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}} = \\frac{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)}}{\\left(\\mathbf{A} \\cdot \\mathbf{p}_{(i)}\\right)^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}}\n",
    "$$\n",
    "\n",
    "The next approximation $\\mathbf{x}_{(i+1)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{(i+1)} = \\mathbf{x}_{(i)} + \\alpha_{(i)} \\cdot \\mathbf{s}_{(i)}\n",
    "$$\n",
    "\n",
    "the next residual $\\mathbf{r}_{(i+1)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_{(i+1)} = \\mathbf{r}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)}\n",
    "$$\n",
    "\n",
    "and the other residual $\\mathbf{s}_{(i+1)}$\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{(i+1)} = \\mathbf{s}_{(i)} - \\alpha_{(i)} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{p}_{(i)} = \\mathbf{A}^T \\cdot \\mathbf{r}_{(i+1)}\n",
    "$$\n",
    "\n",
    "The step size $\\beta_{(i)}$:\n",
    "\n",
    "$$\n",
    "\\beta_{(i)} = \\frac{\\mathbf{s}_{(i+1)}^T \\cdot \\mathbf{s}_{(i+1)}}{\\mathbf{s}_{(i)}^T \\cdot \\mathbf{s}_{(i)} } \n",
    "$$\n",
    "\n",
    "The new search direction $\\mathbf{p}_{(i+1)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{(i+1)} = \\mathbf{s}_{(i+1)} + \\beta_{(i)} \\cdot \\mathbf{p}_{(i)}\n",
    "$$\n",
    "\n",
    "**Termination**\n",
    "\n",
    "A suitable termination criterion may use the quadratic norm of either $\\mathbf{r}_{(i+1)}$ or $\\mathbf{s}_{(i+1)}$ and compare it to an acceptance threshold. The iteration is terminated once the criterion is below that threshold.\n",
    "\n",
    "For each iteration the matrix-vector products\n",
    "\n",
    "$$\\begin{gather}\n",
    " \\mathbf{A} \\cdot \\mathbf{p}_{(i)} \\\\\n",
    "\\mathbf{A} \\cdot \\mathbf{r}_{(i+1)}\n",
    " \\end{gather}\n",
    "$$\n",
    "\n",
    "must be computed.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3e4ee-0f3c-40cb-bee5-637b91d79be4",
   "metadata": {},
   "source": [
    "# Application to a simple problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be614fd8-b940-4f1d-a56c-e22062234987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual : [-0.22742168  0.0944921 ]\n",
      "norm2    : 6.065e-02\n",
      "xvec     : [ 2.1109653  -2.05273712]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 2 x 2 matrix\n",
    "A_mat = np.array([[3.0, 2.0], [2.0, 6.0]])\n",
    "# row column vector)\n",
    "b_vec = np.array([2.0, -8])\n",
    "\n",
    "# Initialisation\n",
    "\n",
    "# starting point x0 (as column vector)\n",
    "x0 = np.array( [0, 0] ).T\n",
    "# the residual (negative gradient vector)\n",
    "r0 = b_vec.T - np.dot(A_mat, x0)\n",
    "\n",
    "# residual s\n",
    "s0 = np.dot(A_mat.T, r0)\n",
    "# the search direction \n",
    "p0 = s0\n",
    "\n",
    "# put x0, r0, d0, s0 into lists\n",
    "xvec_lst = [x0]\n",
    "residual_lst = [r0]\n",
    "p_lst = [p0]\n",
    "res_s_lst = [s0]\n",
    "\n",
    "#----- iterations ------\n",
    "Ni = 2\n",
    "for k in range(Ni):\n",
    "    Amat_p = np.dot(A_mat, p_lst[-1])\n",
    "    # the denominator for the stepsize \n",
    "    alpha_denom = np.dot(Amat_p.T, Amat_p)\n",
    "    # the nominator for the stepsize\n",
    "    alpha_nom = np.dot(p_lst[-1].T, p_lst[-1])\n",
    "    # the stepsize\n",
    "    alpha = alpha_nom/alpha_denom\n",
    "\n",
    "    # new x vector\n",
    "    xn = xvec_lst[-1] + alpha * p_lst[-1]\n",
    "    # new residual\n",
    "    rn = residual_lst[-1] - alpha * Amat_p\n",
    "    # new residual\n",
    "    sn = np.dot(A_mat.T, rn)\n",
    "    # stepsize beta\n",
    "    beta = np.dot(sn.T, sn) / np.dot(res_s_lst[-1].T, res_s_lst[-1])\n",
    "    # new search direction\n",
    "    pn = sn + beta * p_lst[-1]\n",
    "\n",
    "    # updating lists\n",
    "    residual_lst.append(rn)\n",
    "    p_lst.append(pn)\n",
    "    xvec_lst.append(xn)\n",
    "    res_s_lst.append(sn)\n",
    "\n",
    "\n",
    "print(f\"residual : {rn}\")\n",
    "print(f\"norm2    : {np.inner(rn, rn):8.3e}\")\n",
    "print(f\"xvec     : {xn}\")    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2403a5-dbe5-4fe2-b05c-843971096a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
