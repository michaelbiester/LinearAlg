{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef5b40a-c264-46bc-9326-304e100e9d75",
   "metadata": {},
   "source": [
    "# Projections & Orthogonalisation\n",
    "\n",
    "Mainly two resources have been used to setup this notebook:\n",
    "\n",
    "Sources:\n",
    "\n",
    "1) `Linear Algebra : Theory, Intuition, Code` author: Mike X Cohen, publisher: sincXpress\n",
    "\n",
    "2) `No bullshit guide to linear algebra` author: Ivan Savov\n",
    "\n",
    "3) `Matrix Methods for Computational Modeling and Data Analytics` author: Mark Embree, Virginia Tech\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235f2bb-5afd-4d37-8b31-20764a0eec3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Projection (Part 1) / Projection on a vector\n",
    "\n",
    "A vector $\\mathbf{b}$ shall be projected onto another vector $\\mathbf{a}$.\n",
    "\n",
    "$\\mathbf{p} = \\beta \\cdot \\mathbf{a} = proj_{(a)}(b)$ denotes the projection vector. Then vector $\\mathbf{b}$ can be decomposed into the sum of two vectors $\\mathbf{p}$ and $\\mathbf{r}$. \n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\mathbf{p} + \\mathbf{r}\n",
    "$$\n",
    "\n",
    "In computing the projection vector $\\mathbf{p}$ the scalar $\\beta$ shall be chosen such as to <ins>minimise</ins> the norm of the residual vector $\\mathbf{r}$\n",
    "\n",
    "For the  norm $||\\mathbf{r}||$ we get:\n",
    "\n",
    "$$\n",
    "||\\mathbf{r}|| = ||\\mathbf{b} - \\beta \\cdot \\mathbf{a}||\n",
    "$$\n",
    "\n",
    "Minimising $||\\mathbf{r}||$ by proper choice of $\\beta$ is equivalent to minimising the quadratic norm $||\\mathbf{r}||^2$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "||\\mathbf{r}||^2= \\mathbf{r}^T \\cdot \\mathbf{r} = \\left(\\mathbf{b}^T - \\beta \\cdot \\mathbf{a}^T\\right) \\cdot \\left(\\mathbf{b} - \\beta \\cdot \\mathbf{a}\\right) \\\\\n",
    "||\\mathbf{r}||^2 = \\mathbf{b}^T \\cdot \\mathbf{b}  - 2 \\cdot \\beta \\cdot \\mathbf{b}^T \\cdot \\mathbf{a} + \\beta^2 \\cdot \\mathbf{a}^T \\cdot \\mathbf{a}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Differentiating $||\\mathbf{r}||^2$ with respect to $\\beta$ yields:\n",
    "\n",
    "$$\n",
    "\\frac{d||\\mathbf{r}||^2}{d\\beta} = - 2 \\mathbf{b}^T \\cdot \\mathbf{a} + 2 \\cdot \\beta \\cdot \\mathbf{a}^T \\cdot \\mathbf{a}\n",
    "$$\n",
    "\n",
    "The optimum $\\beta$ which minimises $||\\mathbf{r}||$ is therefor:\n",
    "\n",
    "$$\n",
    "\\beta  = \\frac{\\mathbf{b}^T \\cdot \\mathbf{a}}{\\mathbf{a}^T \\cdot \\mathbf{a}}\n",
    "$$\n",
    "\n",
    "Thus we can express vector $\\mathbf{b}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\mathbf{p} + \\mathbf{r} = \\beta \\cdot \\mathbf{a} + \\mathbf{r} = \\underbrace{\\frac{\\mathbf{b}^T \\cdot \\mathbf{a}}{\\mathbf{a}^T \\cdot \\mathbf{a}} \\cdot \\mathbf{a}}_{\\mathbf{p}} + \\mathbf{r}\n",
    "$$\n",
    "\n",
    "The projection vector $\\mathbf{p}$ is in the direction of vector $\\mathbf{a}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\frac{\\mathbf{b}^T \\cdot \\mathbf{a}}{\\mathbf{a}^T \\cdot \\mathbf{a}} \\cdot \\mathbf{a} = \\frac{\\mathbf{b}^T \\cdot \\mathbf{a}}{||\\mathbf{a}||^2} \\cdot \\mathbf{a} = \\mathbf{b}^T \\cdot \\frac{\\mathbf{a}}{||\\mathbf{a}||} \\cdot \\frac{\\mathbf{a}}{||\\mathbf{a}||}\n",
    "$$\n",
    "\n",
    "In this equation the vector $\\frac{\\mathbf{a}}{||\\mathbf{a}||}$ denotes the unit vector in the direction of $\\mathbf{a}$ for which we introduce the notation:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_u = \\frac{\\mathbf{a}}{||\\mathbf{a}||}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\left(\\mathbf{b}^T \\cdot \\mathbf{a}_u\\right) \\cdot \\mathbf{a}_u\n",
    "$$\n",
    "\n",
    "For the residual vector $\\mathbf{r}$ we get:\n",
    "\n",
    "$$\n",
    "\\mathbf{r} = \\mathbf{b} - \\mathbf{p} = \\mathbf{b} - \\left(\\mathbf{b}^T \\cdot \\mathbf{a}_u\\right) \\cdot \\mathbf{a}_u\n",
    "$$\n",
    "\n",
    "**orthogonality of $\\mathbf{r}$ and $\\mathbf{p}$**\n",
    "\n",
    "It shall be shown that $\\mathbf{r}$ is orthogonal to <ins>any</ins> vector $\\alpha \\cdot \\mathbf{a}_u$. We must show that $\\alpha \\cdot \\mathbf{r}^T \\cdot \\mathbf{a}_u = 0$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\alpha \\cdot \\mathbf{r}^T \\cdot \\mathbf{a}_u = \\alpha \\cdot \\mathbf{b}^T \\cdot \\mathbf{a}_u - \\alpha \\cdot \\left(\\mathbf{b}^T \\cdot \\mathbf{a}_u\\right) \\cdot \\mathbf{a}_u^T \\cdot \\mathbf{a}_u \\\\\n",
    "= \\alpha \\cdot \\mathbf{b}^T \\cdot \\mathbf{a}_u - \\alpha \\cdot \\left(\\mathbf{b}^T \\cdot \\mathbf{a}_u\\right) \\cdot ||\\mathbf{a}_u|| \\\\\n",
    "\\alpha \\cdot \\mathbf{r}^T \\cdot \\mathbf{a}_u = \\alpha \\cdot \\mathbf{b}^T \\cdot \\mathbf{a}_u - \\alpha \\cdot \\mathbf{b}^T \\cdot \\mathbf{a}_u = 0 \n",
    "\\end{gather} \n",
    "$$\n",
    "\n",
    "**projectors**\n",
    "\n",
    "The projection $\\mathbf{p}$ of vector $\\mathbf{b}$ onto vector $\\mathbf{a}$ \n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\frac{\\mathbf{b}^T \\cdot \\mathbf{a}}{\\mathbf{a}^T \\cdot \\mathbf{a}} \\cdot \\mathbf{a} \n",
    "$$\n",
    "\n",
    "can be re-arranged. Since the expression $\\frac{\\mathbf{b}^T \\cdot \\mathbf{a}}{\\mathbf{a}^T \\cdot \\mathbf{a}}$ is a scalar we may write:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{p} = \\mathbf{a} \\cdot \\frac{\\mathbf{b}^T \\cdot \\mathbf{a}}{\\mathbf{a}^T \\cdot \\mathbf{a}}  \\\\\n",
    "\\ = \\mathbf{a} \\cdot \\frac{\\mathbf{a}^T \\cdot \\mathbf{b}}{\\mathbf{a}^T \\cdot \\mathbf{a}} \\\\\n",
    "\\ = \\left(\\frac{\\mathbf{a} \\cdot \\mathbf{a}^T }{\\mathbf{a}^T \\cdot \\mathbf{a}}\\right) \\cdot \\mathbf{b}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The expression \n",
    "\n",
    "$\\left(\\frac{\\mathbf{a} \\cdot \\mathbf{a}^T }{\\mathbf{a}^T \\cdot \\mathbf{a}}\\right)$ denotes a square symmetric matrix which only depends on the elements of vector $\\mathbf{a}$. Multiplying this matrix from then right by a vector $\\mathbf{b}$ yields the *best/orthogonal* projection onto vector $\\mathbf{a}$.\n",
    "\n",
    "The matrix is named the projector onto vector $\\mathbf{a}$ and a specific symbol $\\mathbf{P_a} $ is introduced:\n",
    "\n",
    "$$\n",
    "\\mathbf{P_a}  = \\frac{\\mathbf{a} \\cdot \\mathbf{a}^T }{\\mathbf{a}^T \\cdot \\mathbf{a}}\n",
    "$$ \n",
    "\n",
    "Some properties of projectors are summarised here:\n",
    "\n",
    "$\\mathbf{P_a}$ is symmetric. This propery follows from the fact that the matrix is obtained from the outer product of two identical vectors.\n",
    "\n",
    "$\\mathbf{P_a} \\cdot \\mathbf{P_a} = \\mathbf{P_a}$. To see this \n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{P_a} \\cdot \\mathbf{P_a} = \\frac{\\mathbf{a} \\cdot \\mathbf{a}^T}{\\mathbf{a}^T \\cdot \\mathbf{a}} \\cdot \\frac{\\mathbf{a} \\cdot \\mathbf{a}^T }{\\mathbf{a}^T \\cdot \\mathbf{a}} = \\frac{\\mathbf{a} \\cdot \\left(\\mathbf{a}^T \\cdot \\mathbf{a}\\right) \\cdot \\mathbf{a}^T }{\\mathbf{a}^T \\cdot \\mathbf{a} \\cdot \\mathbf{a}^T \\cdot \\mathbf{a}} = \\frac{\\mathbf{a} \\cdot \\mathbf{a}^T }{\\mathbf{a}^T \\cdot \\mathbf{a}} = \\mathbf{P_a}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Another useful identity is:\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{I} - \\mathbf{P_a}\\right) \\cdot \\mathbf{P_a} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "The derivation of this identity uses the property $\\mathbf{P_a} \\cdot \\mathbf{P_a} = \\mathbf{P_a}$:\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{I} - \\mathbf{P_a}\\right) \\cdot \\mathbf{P_a} = \\mathbf{P_a} - \\mathbf{P_a} \\cdot \\mathbf{P_a} = \\mathbf{P_a} - \\mathbf{P_a} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "If vector $\\mathbf{b}$ is already orthogonal to vector $\\mathbf{}$ then \n",
    "\n",
    "$$\n",
    "\\mathbf{P_a} \\cdot \\mathbf{b} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "The expression  \n",
    "\n",
    "$$\\begin{gather}\n",
    "\\left(\\mathbf{I} - \\mathbf{P_a}\\right) \\cdot \\mathbf{b} = \\mathbf{r} \\\\\n",
    "\\to \\\\\n",
    "\\mathbf{r}^T \\cdot \\mathbf{a} = 0\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "is just the residual vector which is orthogonal to $\\mathbf{a}$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The projection $\\mathbf{p}$  of vector $\\mathbf{b}$ onto some vector $\\mathbf{a}$ is computed from this equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\frac{\\mathbf{a}^T \\cdot \\mathbf{b}}{\\mathbf{a}^T \\cdot \\mathbf{a}} \\cdot \\mathbf{a}\n",
    "$$\n",
    "\n",
    "The projector onto $\\mathbf{a}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{P_a}  = \\frac{\\mathbf{a} \\cdot \\mathbf{a}^T }{\\mathbf{a}^T \\cdot \\mathbf{a}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\mathbf{P_a}  \\cdot \\mathbf{a}\n",
    "$$\n",
    "\n",
    "\n",
    "The residual vector $\\mathbf{r}$ is orthogonal to vectors $\\mathbf{a}, \\ \\mathbf{p}$.\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{b} = \\mathbf{p} + \\mathbf{r} \\\\\n",
    "\\to \\\\\n",
    "\\mathbf{r} = \\mathbf{b} - \\mathbf{p} = \\mathbf{b}  - \\frac{\\mathbf{a}^T \\cdot \\mathbf{b}}{\\mathbf{a}^T \\cdot \\mathbf{a}} \\cdot \\mathbf{a}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d89128-465b-4f96-8d9f-eb1577e434a6",
   "metadata": {},
   "source": [
    "## Cauchy-Schwarz Inequality\n",
    "\n",
    "The `Cauchy-Schwarz` inequality states:\n",
    "\n",
    "$$\n",
    "|\\mathbf{v}^T \\cdot \\mathbf{w}| \\le ||\\mathbf{v}|| \\cdot ||\\mathbf{w}||\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "We look at the quadratic norm\n",
    "\n",
    "$$\\begin{gather}\n",
    "||\\mathbf{w} + \\alpha \\cdot \\mathbf{v}||^2 = \\mathbf{w}^T \\cdot \\mathbf{w} + 2 \\cdot \\alpha \\cdot \\mathbf{w}^T \\cdot \\mathbf{v} + \\alpha^2 \\cdot \\mathbf{v}^T \\cdot \\mathbf{v} \\\\\n",
    "0 \\le  ||\\mathbf{w}||^2 + 2 \\cdot \\alpha \\cdot \\mathbf{w}^T \\cdot \\mathbf{v} + \\alpha^2 \\cdot ||\\mathbf{v}||^2\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The right side of the inequality describes a parabola $f(\\alpha)$ which must be strictly non-negative. Thus solutions $\\alpha$ to $f(\\alpha) = 0$ must be complex.\n",
    "\n",
    "$$\\begin{gather}\n",
    "f(\\alpha) = ||\\mathbf{w}||^2 + 2 \\cdot \\alpha \\cdot \\mathbf{w}^T \\cdot \\mathbf{v} + \\alpha^2 \\cdot ||\\mathbf{v}||^2\n",
    "\\end{gather} \\\\\n",
    "f(\\alpha) = ||\\mathbf{v}||^2 \\cdot \\left(\\alpha^2 + 2 \\cdot \\alpha \\cdot \\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2} + \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}||^2}  \\right)\n",
    "$$\n",
    "\n",
    "Ignoring (for a moment) the case $\\mathbf{v} = \\mathbf{0}$ we are looking for those values $\\alpha$ for which $f(\\alpha) \\ge 0$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "0 \\le \\alpha^2 + 2 \\cdot \\alpha \\cdot \\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2} + \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}||^2} \\\\\n",
    "\\left(\\alpha + \\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2}\\right)^2 + \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}||^2} - \\left(\\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2}\\right)^2 \\ge 0 \\\\\n",
    "\\left(\\alpha + \\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2}\\right)^2 \\ge \\left(\\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2}\\right)^2 - \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}||^2} \\\\\n",
    "\\alpha + \\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2} \\ge \\sqrt{\\left(\\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2}\\right)^2 - \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}||^2}}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "For complex zeros $\\alpha$ we need:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\left(\\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2}\\right)^2 - \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}||^2} \\le 0 \\\\\n",
    "\\left(\\frac{\\mathbf{w}^T \\cdot \\mathbf{v}}{||\\mathbf{v}||^2}\\right)^2 \\le \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}||^2} \\\\\n",
    "\\frac{\\left(\\mathbf{w}^T \\cdot \\mathbf{v}\\right)^2}{||\\mathbf{v}||^4} \\le \\frac{||\\mathbf{w}||^2}{||\\mathbf{v}^2||}  \\\\\n",
    "|\\mathbf{w}^T \\cdot \\mathbf{v}|^2 \\le ||\\mathbf{w}||^2 \\cdot ||\\mathbf{v}||^2 \\\\ \n",
    "\\to \\\\\n",
    "|\\mathbf{w}^T \\cdot \\mathbf{v}| \\le ||\\mathbf{w}|| \\cdot ||\\mathbf{v}||\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The last equation completes the proof. We have ignored the case $\\mathbf{v} = \\mathbf{0}$. But in this case we obviously have:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\cdot \\mathbf{v} = 0 = ||\\mathbf{w}|| \\cdot ||\\mathbf{v}||\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270b50c-123b-42aa-beb5-114070e5efae",
   "metadata": {},
   "source": [
    "## Triangle Inequality\n",
    "\n",
    "The `Triangel Inequality` states:\n",
    "\n",
    "$$\n",
    "||\\mathbf{v} + \\mathbf{w}|| \\le ||\\mathbf{v}|| + ||\\mathbf{w}||\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Instead of dealing directly with the norm we look at the squared norm which is easier to evaluate.\n",
    "\n",
    "$$\n",
    "||\\mathbf{v} + \\mathbf{w}||^2 = ||\\mathbf{v}||^2 + 2 \\cdot \\mathbf{w}^T \\cdot \\mathbf{v} + ||\\mathbf{w}||^2\n",
    "$$\n",
    "\n",
    "From the `Cauchy-Schwarz` inequality we know:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\cdot \\mathbf{v} \\le |\\mathbf{w}^T \\cdot \\mathbf{v}| \\le ||\\mathbf{w}|| \\cdot ||\\mathbf{v}||\\\n",
    "$$\n",
    "\n",
    "$$\\begin{gather}\n",
    "||\\mathbf{v} + \\mathbf{w}||^2 \\le ||\\mathbf{v}||^2 + 2 \\cdot ||\\mathbf{w}|| \\cdot ||\\mathbf{v}|| + ||\\mathbf{w}||^2 = \\left(||\\mathbf{v}||+ ||\\mathbf{w}|| \\right)^2 \\\\\n",
    "\\to \\\\\n",
    "||\\mathbf{v} + \\mathbf{w}|| \\le ||\\mathbf{v}||+ ||\\mathbf{w}||\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The last equation completes the proof.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636786ff-a3ad-40eb-b62c-e1dc6cd89c30",
   "metadata": {},
   "source": [
    "## Projections with more than one vector\n",
    "\n",
    "Matrix $\\mathbf{A}$ is of type $m \\times n$ and vector $\\mathbf{x}$ has $n$ elements. Thus the product $\\mathbf{A} \\cdot \\mathbf{x}$  is defined.\n",
    "\n",
    "It follows from the columns perspective that the matrix vector product $\\mathbf{A} \\cdot \\mathbf{x}$ is a `m`-element column vector which is a linear combination of column vectors of matrix $\\mathbf{A}$ with weighting / scaling factors being elements of vector $\\mathbf{x}$.\n",
    "\n",
    "An arbitrarily chosen `m`-element columns vector $\\mathbf{b}$ shall be *approximated* by $\\mathbf{A} \\cdot \\mathbf{x}$. Defining a residual vector $\\mathbf{r}$ by\n",
    "\n",
    "$$\n",
    "\\mathbf{r} = \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Ideally we would have $\\mathbf{r} = \\mathbf{0}$. But that is only possible if $\\mathbf{b}$ is in the subspace spanned by the columns of matrix $\\mathbf{A}$. Apart from this special case we have $\\mathbf{r} \\neq \\mathbf{0}$ regardless of the choice of vector $\\mathbf{x}$.\n",
    "\n",
    "A more *relaxed* requirement is to demand that vector $\\mathbf{r}$ shall be orthogonal to each column of matrix $\\mathbf{A}$. Thus we require:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^T \\cdot \\underbrace{\\left( \\mathbf{b} - \\mathbf{A} \\cdot \\mathbf{x} \\right)}_{\\mathbf{r}} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "The equation above is transformed in a couple of step in something that is easier to interpret.\n",
    "\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{A}^T \\cdot \\mathbf{b} - \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{0} \\\\\n",
    "\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "\\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\right)^{-1} \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} = \\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\right)^{-1} \\cdot \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "\\to \\\\\n",
    "\\mathbf{x} = \\underbrace{\\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\right)^{-1} \\cdot \\mathbf{A}^T}_{left \\ inverse} \\cdot \\mathbf{b}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The left inverse can be computed if these conditions are fulfilled:\n",
    "\n",
    "1) $\\mathbf{A}$ is square and is full rank. Then $\\mathbf{A}^T \\cdot \\mathbf{A}$ is also full rank and has an inverse\n",
    "\n",
    "2) $\\mathbf{A}$ is a `tall` matrix with full column rank\n",
    "\n",
    "\n",
    "**How to solve it**\n",
    "\n",
    "1) directly apply the formula of the left matrix inverse\n",
    "\n",
    "2) more elegantly use a routine from `numpy`. `numpy.linalg.lstsq`\n",
    "\n",
    "The code blocks below demonstrate both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2331acdc-f80c-4ce8-96e7-bf37ba56198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# a random 4 x 2 matrix\n",
    "Amat = np.random.randn(4, 2)\n",
    "# a random 4 element column vector\n",
    "bvec = np.random.randn(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff4384b-42b2-4677-b194-19d301d8a74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct method -> xvec_m1 : [0.50635871 0.68656467]\n"
     ]
    }
   ],
   "source": [
    "# computing the left inverse (see formula)\n",
    "\n",
    "Ileft = np.linalg.inv(Amat.T @ Amat) @ Amat.T\n",
    "xvec_m1 = Ileft @ bvec\n",
    "print(f\"direct method -> xvec_m1 : {xvec_m1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3eae72e-7670-4b32-9c2f-68320536b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least square method -> xvec_m2 : [0.50635871 0.68656467]\n",
      "\n",
      "residuals : [0.27119096]\n"
     ]
    }
   ],
   "source": [
    "# computing from least-squares\n",
    "\n",
    "xvec_m2, residuals_1, rank, singular_values = np.linalg.lstsq(Amat, bvec, rcond=None)\n",
    "print(f\"least square method -> xvec_m2 : {xvec_m2}\\n\")\n",
    "print(f\"residuals : {residuals_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435d5e29-10b7-42cc-ae5c-f77c8afdf399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residuals : 0.2711909625466214\n"
     ]
    }
   ],
   "source": [
    "# another way to compute the residuals\n",
    "# -> quite similar values ...\n",
    "\n",
    "r = bvec - Amat @ xvec_m2\n",
    "residuals_2 = np.linalg.norm(r)**2\n",
    "print(f\"residuals : {residuals_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d1eb7-469c-42c4-8783-f2cba8b5f616",
   "metadata": {},
   "source": [
    "## Orthogonal matrices\n",
    "\n",
    "Properties of orthogonal matrices:\n",
    "\n",
    "1) column vectors are orthogonal; `i'th` column is orthogonal to `j'th` column for $i \\neq j$.\n",
    "\n",
    "2) all columns have length 1\n",
    "\n",
    "The orthogonality / orthonormality of column vectors is summarized in this matrix product:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}^T \\cdot \\mathbf{Q} = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{Q}$ is square it has a left- and right-sided inverse.\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}^{-1} = \\mathbf{Q}^T\n",
    "$$\n",
    "\n",
    "For a tall matrix with orthonormal column vectors only the left-sided inverse exists.\n",
    "\n",
    "No inverse is defined if the matrix is wide.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0004263-4f33-4dd4-9828-70aee21966e2",
   "metadata": {},
   "source": [
    "## Projections (Part 2)\n",
    "\n",
    "**orthogonal projection**\n",
    "\n",
    "For orthogonal vectors $\\mathbf{q}_1,\\ \\mathbf{q}_2, \\ldots ,  \\mathbf{q}_n$ the projection vector $\\mathbf{p}$ of vector $\\mathbf{b}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\frac{\\mathbf{b}^T \\mathbf{q}_1}{\\mathbf{q}_1^T \\mathbf{q}_1 } \\cdot \\mathbf{q}_1 + \\frac{\\mathbf{b}^T \\mathbf{q}_2}{\\mathbf{q}_2^T \\mathbf{q}_2 } \\cdot \\mathbf{q}_2 + \\cdots + \\frac{\\mathbf{b}^T \\mathbf{q}_n}{\\mathbf{q}_n^T \\mathbf{q}_n } \\cdot \\mathbf{q}_n \n",
    "$$\n",
    "\n",
    "The residual vector $\\mathbf{r} = \\mathbf{b} - \\mathbf{p}$ is then orthogonal to each vector $\\mathbf{q}_1,\\ \\mathbf{q}_2, \\ldots ,  \\mathbf{q}_n$.\n",
    "\n",
    "**proof**\n",
    "\n",
    "It must be shown that $\\mathbf{q}_j^T  \\cdot \\mathbf{r} = 0$.\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{q}_j^T  \\cdot \\mathbf{r} = \\mathbf{q}_j^T \\cdot \\mathbf{b} - \\mathbf{q}_j^T \\cdot \\mathbf{p} \\\\\n",
    "\\mathbf{q}_j^T  \\cdot \\mathbf{r} = \\mathbf{q}_j^T \\cdot \\mathbf{b} - \\frac{\\mathbf{b}^T \\mathbf{q}_1}{\\mathbf{q}_1^T \\mathbf{q}_1 } \\cdot \\mathbf{q}_j^T \\cdot \\mathbf{q}_1 + \\mathbf{q}_j^T \\cdot \\frac{\\mathbf{b}^T \\mathbf{q}_2}{\\mathbf{q}_2^T \\mathbf{q}_2 } \\cdot \\mathbf{q}_j^T \\cdot \\mathbf{q}_2 + \\cdots + \\mathbf{q}_j^T \\cdot \\frac{\\mathbf{b}^T \\mathbf{q}_n}{\\mathbf{q}_n^T \\mathbf{q}_n } \\cdot \\mathbf{q}_j^T \\cdot \\mathbf{q}_n \\\\\n",
    "\\mathbf{q}_j^T  \\cdot \\mathbf{r} = \\mathbf{q}_j^T \\cdot \\mathbf{b} - \\frac{\\mathbf{b}^T \\mathbf{q}_j}{\\mathbf{q}_j^T \\mathbf{q}_j } \\cdot \\mathbf{q}_j^T \\cdot \\mathbf{q}_j = \\mathbf{q}_j^T \\cdot \\mathbf{b} - \\mathbf{b}^T \\mathbf{q}_j = 0\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab990996-b457-466c-ba92-f644d2dc9cc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Computing an orthogonal basis\n",
    "\n",
    "also known as `Gram-Schmidt` procedure.\n",
    "\n",
    "Two very readable accounts I found here:\n",
    "\n",
    "1) `QR Decomposition with Gram-Schmidt` , Igor Yanovsky (Math 151B TA)\n",
    "\n",
    "2) `Lecture 4: Applications of Orthogonality: QR Decomposition`, author: Padraic Bartlett, UCSB 2014\n",
    "   \n",
    "\n",
    "A matrix $\\mathbf{A}$ has `n` column vectors $\\mathbf{a}_1, \\mathbf{a}_2, \\ldots , \\mathbf{a}_n$. These column vectors are linearly independent and span a vector space. The column vectors are in general not orthogonal / orthonomal. \n",
    "\n",
    "**Task**\n",
    "\n",
    "1) Derive a set of orthogonal vectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots , \\mathbf{u}_n$ which span the same vector space.\n",
    "\n",
    "2) The set of orthogonal vectors shall be constructed from the column vectors of matrix $\\mathbf{A}$.\n",
    "\n",
    "3) normalise the set of orthogonal vectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots , \\mathbf{u}_n$ to obtain a set of <ins>orthonormal</ins> vectors $\\mathbf{q}_1, \\mathbf{q}_2, \\ldots , \\mathbf{q}_n$ .\n",
    "\n",
    "The orthogonal vectors are constructed from a series of `n` steps. Each steps generates a (the next) orthonormal vector which is used in subsequent steps.\n",
    "\n",
    "<ins>step#1</ins>\n",
    "\n",
    "Take the first column vector $\\mathbf{a}_1$ as orthogonal vector $\\mathbf{u}_1$.\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_1 = \\mathbf{a}_1\n",
    "$$\n",
    "\n",
    "<ins>step#2</ins>\n",
    "\n",
    "using $\\mathbf{u}_1$ and the projection theorem it is known that the residual $\\mathbf{u}_2$ is orthogonal to $\\mathbf{u}_1$. \n",
    "\n",
    "$$\n",
    "\\mathbf{u}_2 = \\mathbf{a}_2 - \\frac{\\mathbf{a}_2 \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} \\cdot \\mathbf{u}_1 \n",
    "$$\n",
    "\n",
    "$\\mathbf{u}_2$ is orthogonal to $\\mathbf{u}_1$\n",
    "\n",
    "<ins>step#3</ins>\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_3 = \\mathbf{a}_3 - \\frac{\\mathbf{a}_3 \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} \\cdot \\mathbf{u}_1 - \\frac{\\mathbf{a}_3 \\cdot \\mathbf{u}_2}{\\mathbf{u}_2^T \\cdot \\mathbf{u}_2} \\cdot \\mathbf{u}_2\n",
    "$$\n",
    "\n",
    "$\\mathbf{u}_3$ is orthogonal to $\\mathbf{u}_1$ and $\\mathbf{u}_2$\n",
    "\n",
    "<ins>step#(k+1)</ins> `(k > 1)`\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_{(k+1)} = \\mathbf{a}_{(k+1)} - \\sum_{i=1}^{k}\\frac{\\mathbf{a}_{(k+1)} \\cdot \\mathbf{u}_i}{\\mathbf{u}_i^T \\cdot \\mathbf{u}_i} \\cdot \\mathbf{u}_i \n",
    "$$\n",
    "\n",
    "Repeating these step up to $k+1 = n$ the complete set of orthogonal vectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots , \\mathbf{u}_n$ has been found. By normalising these vectors using $\\mathbf{q}_i = \\frac{\\mathbf{u}_i}{||\\mathbf{u}_i||} = \\frac{\\mathbf{u}_i}{\\sqrt{\\mathbf{u}_i^T \\cdot \\mathbf{u}_i}}$ the set of <ins>orthonormal</ins> vectors $\\mathbf{q}_1, \\mathbf{q}_2, \\ldots , \\mathbf{q}_n$ is generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22510e-4b3a-418f-b266-ac30b32b511a",
   "metadata": {},
   "source": [
    "## QR Decomposition\n",
    "\n",
    "From the orthogonalisation procedure we express the column vectors $\\mathbf{a}_k$ :\n",
    "\n",
    "<ins>step#1</ins>\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_1 = \\mathbf{u}_1\n",
    "$$\n",
    "\n",
    "<ins>step#(k+1)</ins> `(k > 1)`\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_{(k+1)} = \\mathbf{u}_{(k+1)} + \\sum_{i=1}^{k}\\frac{\\mathbf{a}_{(k+1)} \\cdot \\mathbf{u}_i}{\\mathbf{u}_i^T \\cdot \\mathbf{u}_i} \\cdot \\mathbf{u}_i \n",
    "$$\n",
    "\n",
    "The column vectors $\\mathbf{a}_i$ are therefore expressed as weigthed additions of the orthogonal vectors $\\mathbf{a}_{(k+1)}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \n",
    "\\left[\\begin{array}{ccccc}\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\mathbf{u}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 & \\ldots & \\mathbf{u}_n \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\end{array}\\right] \\cdot\n",
    "\\left[\\begin{array}{ccccc}\n",
    "1 & \\frac{\\mathbf{a}_{2} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} & \\frac{\\mathbf{a}_{3} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} & \\ldots & \\frac{\\mathbf{a}_{n} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} \\\\\n",
    "0 & 1 & \\frac{\\mathbf{a}_{3} \\cdot \\mathbf{u}_2}{\\mathbf{u}_2^T \\cdot \\mathbf{u}_2} & \\ldots & \\frac{\\mathbf{a}_{n} \\cdot \\mathbf{u}_2}{\\mathbf{u}_2^T \\cdot \\mathbf{u}_2} \\\\\n",
    "0 & 0 & 1 & \\ldots & \\frac{\\mathbf{a}_{n} \\cdot \\mathbf{u}_3}{\\mathbf{u}_3^T \\cdot \\mathbf{u}_3} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "We observe that matrix $\\mathbf{A}$ is the product of two matrices. The left matrix has mutual orthogonal column vectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots , \\mathbf{u}_n$ while the right matrix is a upper triangular matrix of *weighting* factors.\n",
    "\n",
    "The final step involves multiplication of the column vectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots , \\mathbf{u}_n$ to obtain the set of orthonormal vectors $\\mathbf{q}_1=\\frac{1}{||\\mathbf{u}_1||} \\cdot \\mathbf{u}_1, \\mathbf{q}_2=\\frac{1}{||\\mathbf{u}_2||} \\cdot \\mathbf{u}_2, \\ldots , \\mathbf{q}_n=\\frac{1}{||\\mathbf{u}_n||} \\cdot \\mathbf{u}_n$\n",
    "\n",
    "To compensate for this scaling the columns of the upper triangular matrix must be scaled as well. The first row vector is scaled by $||\\mathbf{u}_1||$. The second row is scaled by $||\\mathbf{u}_2||$ and so on. After application of these scaling operations matrix $\\mathbf{A}$ is expressed like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q} \\cdot \\mathbf{R} =\n",
    "\\left[\\begin{array}{ccccc}\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\mathbf{q}_1 & \\mathbf{q}_2 & \\mathbf{q}_3 & \\ldots & \\mathbf{q}_n \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\end{array}\\right] \\cdot\n",
    "\\left[\\begin{array}{ccccc}\n",
    "||\\mathbf{u}_1|| & ||\\mathbf{u}_1|| \\cdot \\frac{\\mathbf{a}_{2} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} & ||\\mathbf{u}_1|| \\cdot \\frac{\\mathbf{a}_{3} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} & \\ldots & ||\\mathbf{u}_1|| \\cdot \\frac{\\mathbf{a}_{n} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1^T \\cdot \\mathbf{u}_1} \\\\\n",
    "0 & ||\\mathbf{u}_2|| & ||\\mathbf{u}_2|| \\cdot \\frac{\\mathbf{a}_{3} \\cdot \\mathbf{u}_2}{\\mathbf{u}_2^T \\cdot \\mathbf{u}_2} & \\ldots & ||\\mathbf{u}_2|| \\cdot \\frac{\\mathbf{a}_{n} \\cdot \\mathbf{u}_2}{\\mathbf{u}_2^T \\cdot \\mathbf{u}_2} \\\\\n",
    "0 & 0 & ||\\mathbf{u}_3|| & \\ldots & ||\\mathbf{u}_3|| \\cdot \\frac{\\mathbf{a}_{n} \\cdot \\mathbf{u}_3}{\\mathbf{u}_3^T \\cdot \\mathbf{u}_3} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & ||\\mathbf{u}_n|| \\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The last matrix product is known as `QR-Decomposition`.\n",
    "\n",
    "A slightly different form is obtained by transforming the upper triangular matrix.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q} \\cdot \\mathbf{R} =\n",
    "\\left[\\begin{array}{ccccc}\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\mathbf{q}_1 & \\mathbf{q}_2 & \\mathbf{q}_3 & \\ldots & \\mathbf{q}_n \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\end{array}\\right] \\cdot\n",
    "\\left[\\begin{array}{ccccc}\n",
    "||\\mathbf{u}_1|| &  \\mathbf{a}_{2} \\cdot \\mathbf{q}_1 & \\mathbf{a}_{3} \\cdot \\mathbf{q}_1 & \\ldots & \\mathbf{a}_{n} \\cdot \\mathbf{q}_1 \\\\\n",
    "0 & ||\\mathbf{u}_2|| & \\mathbf{a}_{3} \\cdot \\mathbf{q}_2 & \\ldots & \\mathbf{a}_{n} \\cdot \\mathbf{q}_2 \\\\\n",
    "0 & 0 & ||\\mathbf{u}_3|| & \\ldots & \\mathbf{a}_{n} \\cdot \\mathbf{q}_3 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & ||\\mathbf{u}_n|| \\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24059fb-5a7a-4fde-8c3a-3bd7e5988fd5",
   "metadata": {},
   "source": [
    "## A numerical example of QR decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0ff427-1cad-4ab1-bc30-e0d95b1b0b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amat   :\n",
      "[[ 2  1  3  3]\n",
      " [ 2  1 -1  1]\n",
      " [ 2 -1  3 -3]\n",
      " [ 2 -1 -1 -1]]\n",
      "\n",
      "Qmat   :\n",
      "[[-0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5  0.5  0.5]\n",
      " [-0.5  0.5 -0.5  0.5]\n",
      " [-0.5  0.5  0.5 -0.5]]\n",
      "\n",
      "Rmat   :\n",
      "[[-4.  0. -2.  0.]\n",
      " [ 0. -2.  0. -4.]\n",
      " [ 0.  0. -4.  0.]\n",
      " [ 0.  0.  0. -2.]]\n",
      "\n",
      "Amat_c :\n",
      "[[ 2.  1.  3.  3.]\n",
      " [ 2.  1. -1.  1.]\n",
      " [ 2. -1.  3. -3.]\n",
      " [ 2. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "Amat = np.array([[2, 1, 3, 3], [2, 1, -1, 1], [2, -1, 3, -3], [2, -1, -1, -1]])\n",
    "\n",
    "Qmat, Rmat = np.linalg.qr(Amat, mode='complete')\n",
    "\n",
    "# compute Amat from Qmat, Rmat as a sanity check (should be identical apart from rounding errors)\n",
    "Amat_c = Qmat @ Rmat\n",
    "\n",
    "print(f\"Amat   :\\n{Amat}\\n\")\n",
    "print(f\"Qmat   :\\n{Qmat}\\n\")\n",
    "print(f\"Rmat   :\\n{Rmat}\\n\")\n",
    "print(f\"Amat_c :\\n{Amat_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d3630-2ac1-45e5-819e-e61a9a5fca80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Application of QR decomposition\n",
    "\n",
    "The matrix equation \n",
    "\n",
    "$$ \n",
    "\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "shall be solved using the `QR-decompostion`. (assuming that $\\mathbf{A}$ has an inverse)\n",
    "\n",
    "Re-writing the matrix equation \n",
    "\n",
    "$$ \n",
    "\\mathbf{Q} \\cdot \\mathbf{R} \\cdot \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "and left multiplying both sides by $\\mathbf{Q}^T$ yields:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{Q}^T \\cdot \\mathbf{Q} \\cdot \\mathbf{R} \\cdot \\mathbf{x} = \\mathbf{Q}^T \\cdot \\mathbf{b} \\\\\n",
    "\\mathbf{R} \\cdot \\mathbf{x} = \\mathbf{Q}^T \\cdot \\mathbf{b} = \\mathbf{c}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The fact that $\\mathbf{R}$ is an upper-triangular matrix makes computation of the elements of vector $\\mathbf{x}$ fairly easy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b932d-3e66-47c8-8c93-0d74d987b781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
