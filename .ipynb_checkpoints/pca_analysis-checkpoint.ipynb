{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9f1c71-77b1-4960-8733-45c2b6ed63df",
   "metadata": {},
   "source": [
    "# Principal Component Analyis\n",
    "\n",
    "**Literature**\n",
    "\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "Getting an idea how `PCA` works and what kind of problems can be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c53e5-985e-4194-92ca-3dc46783a2ad",
   "metadata": {},
   "source": [
    "##  Background\n",
    "\n",
    "1) A measurement is defined as a collection of `K` items.\n",
    "\n",
    "2) In general there are `N` measurement\n",
    "\n",
    "3) The `j-th` measurement has `K` measured items. These items are arranged into a column vector denoted $\\mathbf{d}_j$.\n",
    "\n",
    "$$\n",
    "\\mathbf{d}_j = \\left[\\begin{array}{c}\n",
    "d_{1}[j]  \\\\ \\vdots \\\\ d_{k}[j] \\\\ \\vdots \\\\ d_{K}[j]\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "d_{1,\\ j} \\\\  \\vdots \\\\ d_{k, \\ j} \\\\ \\vdots \\\\ d_{K,\\ j}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$d_{k}[j] = d_{k\\ j} $ denotes the `j-th` measurement / data of the `k-th` item.\n",
    "\n",
    "---\n",
    "\n",
    "**centering the data set**\n",
    "\n",
    "The mean value of the data set is computed from all measurements $\\mathbf{d}_j : \\ j=1,  \\ldots, N$ by taking the *element-wise* average of each measurement item. The mean value is defined as a column vector:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{d}) = \\left[\\begin{array}{c}\n",
    "E(d_{1}) \\\\\n",
    "\\vdots \\\\\n",
    "E(d_{k}) \\\\\n",
    "\\vdots \\\\\n",
    "E(d_{K})\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "\\frac{1}{N} \\sum_{j=1}^N d_{1, j} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{1}{N} \\sum_{j=1}^N d_{k, j} \\\\\n",
    "\\cdots \\\\\n",
    "\\frac{1}{N} \\sum_{j=1}^N d_{K, j}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The **centered** data vector are denoted $\\mathbf{x}_j : \\ j =1, \\ldots, N$. They are computed from data vectors $\\mathbf{d}_j$ by removing the mean value.\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_j = \\left[\\begin{array}{c}\n",
    "d_{1}[j] - E(d_{1})   \\\\ \\vdots \\\\ d_{k}[j] - E(d_{k})\\\\ \\vdots \\\\ d_{K}[j] - E(d_{K})\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "d_{1,\\ j} - E(d_{1})\\\\  \\vdots \\\\ d_{k,\\ j} - E(d_{k})\\\\ \\vdots \\\\ d_{K,\\ j} - E(d_{K})\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "x_{1,\\ j}\\\\  \\vdots \\\\ x_{k,\\ j}\\\\ \\vdots \\\\ x_{K,\\ j}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Now we define a unit vector $\\mathbf{w} : \\ \\in \\mathbb{R}^{K} ; \\ ||\\mathbf{w}|| = 1$. This vector shall be to determine the component of the `j-th` measurement in the direction of vector  $\\mathbf{w}$. This vector is denoted $\\mathbf{p}_j$. It is the projection of vector $\\mathbf{x}_j$ onto $\\mathbf{w}$. The residual vector $\\mathbf{r}_j$ is orthogonal to vector $\\mathbf{p}_j$.\n",
    "\n",
    "To summarise (projection vector & residual vector):\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{p}_j = \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}\\right) \\cdot \\mathbf{w}  \\\\\n",
    "\\ \\\\\n",
    "\\mathbf{r}_j = \\mathbf{x}_j - \\mathbf{p}_j \\\\\n",
    "\\mathbf{r}_j = \\mathbf{x}_j - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}\\right) \\cdot \\mathbf{w}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The squared norm of the residual $\\mathbf{r}_j$ is computed from:\n",
    "\n",
    "$$\\begin{gather}\n",
    "||\\mathbf{r}_j||^2 = \\mathbf{r}_j^T \\cdot \\mathbf{r}_j = \\left(\\mathbf{x}_j - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}\\right) \\cdot \\mathbf{w} \\right)^T \\cdot \\left(\\mathbf{x}_j - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}\\right) \\cdot \\mathbf{w} \\right) \\\\\n",
    "\\ = \\mathbf{x}_j^T \\cdot \\mathbf{x}_j - \\mathbf{x}_j^T \\cdot \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}\\right) \\cdot \\mathbf{w}  - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\cdot \\mathbf{w} \\right)^T \\cdot \\mathbf{x}_j  + \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\cdot \\mathbf{w} \\right)^T \\cdot \\mathbf{x}_j^T \\cdot \\mathbf{w} \\cdot \\mathbf{w} \\\\\n",
    "\\ = \\mathbf{x}_j^T \\cdot \\mathbf{x}_j - 2 \\cdot \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\right)^2 + \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\right)^2 \\\\\n",
    "\\ = \\mathbf{x}_j^T \\cdot \\mathbf{x}_j - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\right)^2 = ||\\mathbf{x}_j||^2 - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\right)^2\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "We have used the unit vector property of vector $\\mathbf{w}$ namely $||\\mathbf{w}|| = 1$.\n",
    "\n",
    "**adding up the squared residual**\n",
    "\n",
    "The mean squared error is the expectation of the squared residuals for all `N` measurements.\n",
    "\n",
    "$$\n",
    "MSE(\\mathbf{w}) = \\frac{1}{N} \\sum_{j=1}^{N} ||\\mathbf{x}_j||^2 - \\frac{1}{N} \\sum_{j=1}^{N} \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\right)^2\n",
    "$$\n",
    "\n",
    "To make $MSE(\\mathbf{w})$ as small as possible the term\n",
    "\n",
    "$$\n",
    "V = \\frac{1}{N} \\sum_{j=1}^{N} \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w} \\right)^2\n",
    "$$\n",
    "\n",
    "must be maximised by appropriate choice of vector $\\mathbf{w}$. Re-writing $V$ yields:\n",
    "\n",
    "\n",
    "$$\n",
    "V = \\frac{1}{N} \\sum_{j=1}^{N} \\mathbf{w}^T \\cdot \\underbrace{\\left(\\mathbf{x}_j \\cdot \\mathbf{x}_j^T \\right)}_{\\mathbf{X}_j} \\cdot \\mathbf{w} = \\mathbf{w}^T \\cdot \\underbrace{\\left( \\frac{1}{N} \\sum_{j=1}^{N} \\mathbf{X}_j  \\right)}_{\\mathbf{X}} \\cdot \\mathbf{w} = \\mathbf{w}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "$\\mathbf{X}_j = \\mathbf{x}_j \\cdot \\mathbf{x}_j^T \\ : \\ \\in \\mathbb{R}^{K \\times K}$ is a square matrix computed as the outer product of centered data vector $\\mathbf{x}_j $.\n",
    "\n",
    "$\\mathbf{X}_j $ has elements $x_{l,\\ m}[j]$.\n",
    "\n",
    "\n",
    "$$\\begin{gather}\n",
    "x_{l,\\ m}[j] = x_{l}[j] \\cdot x_{m}[j] = \\left(d_{l}[j] - E(d_{l}) \\right) \\cdot \\left(d_{m}[j] - E(d_{m}) \\right) \\\\\n",
    "x_{l,\\ m}[j] = d_{l}[j] \\cdot d_{m}[j] - d_{l}[j] \\cdot E(d_{m}) - d_{m}[j] \\cdot E(d_{l}) +  E(d_{l}) \\cdot  E(d_{m})\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "The sum of all matrices $\\mathbf{X}_j$ is matrix $\\mathbf{X}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\frac{1}{N} \\sum_{j=1}^{N} \\mathbf{X}_j \n",
    "$$\n",
    "\n",
    "with elements $x_{l,\\ m}$:\n",
    "\n",
    "$$\\begin{gather}\n",
    "x_{l,\\ m} = E(d_{l} \\cdot d_{m}) - E(d_{m}) \\cdot E(d_{l})  - E(d_{m}) \\cdot E(d_{l}) +  E(d_{l}) \\cdot  E(d_{m}) \\\\\n",
    "\\ \\\\\n",
    "x_{l,\\ m} = E(d_{l} \\cdot d_{m}) - E(d_{m}) \\cdot E(d_{l})  \\\\\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "\n",
    "For $l=m$ matrix elements $x_{l,\\ l}$ are just the variance:\n",
    "\n",
    "$$\n",
    "x_{l,\\ l} = E(d_{l} \\cdot d_{l})- E(d_{l})^2 = E(d_{l}^2)- E(d_{l})^2\n",
    "$$\n",
    "\n",
    "For  $l \\neq m$ matrix elements $x_{l,\\ m}$ are the covariance.\n",
    "\n",
    "Hence matrix $\\mathbf{X}$ is also known as the <ins>covariance</ins> matrix.\n",
    "\n",
    "**summary**\n",
    "\n",
    "It has been demonstrated that by appropriate choice of projection vector $\\mathbf{w}$ the maximisation of \n",
    "\n",
    "$$\n",
    "V = \\mathbf{w}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "minimises the mean squared error $MSE(\\mathbf{w})$.\n",
    "\n",
    "The next step is to determine the optimum vector $\\mathbf{w}$ with the constraint that $\\mathbf{w}$ has unit length ($||\\mathbf{w}|| = 1$).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2bed2d-145b-4878-8480-b8285d3202ea",
   "metadata": {},
   "source": [
    "## Finding the vector minimising the MSE\n",
    "\n",
    "The optimum vector must be computed with the unit length constraint.\n",
    "\n",
    "Using the `Lagrange` multiplier method, the objective function $F(\\mathbf{w}, \\lambda)$ may be expressed like this:\n",
    "\n",
    "$$\n",
    "F(\\mathbf{w}, \\lambda) = \\mathbf{w}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w} - \\lambda \\cdot \\left(\\mathbf{w}^T \\cdot \\mathbf{w} -1 \\right)\n",
    "$$\n",
    "\n",
    "**ToDo**\n",
    "\n",
    "Get a solid understanding of constrained optimisation with `Lagrange` multipliers. (in this notebook I just copied the method without having understood how it works)\n",
    "\n",
    "---\n",
    "\n",
    "The solution vector is found by setting derivatives\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\frac{\\partial}{\\partial \\mathbf{w} } F(\\mathbf{w}, \\lambda) = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\lambda } F(\\mathbf{w}, \\lambda) = 0 \n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{w} } F(\\mathbf{w}, \\lambda) = 2 \\cdot \\mathbf{X} \\cdot \\mathbf{w} - 2 \\cdot \\lambda \\cdot \\mathbf{w} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\lambda } F(\\mathbf{w}, \\lambda) = \\mathbf{w}^T \\cdot \\mathbf{w} -1\n",
    "$$\n",
    "\n",
    "Leading to \n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{X} \\cdot \\mathbf{w} = \\lambda \\cdot \\mathbf{w} \\\\\n",
    "\\mathbf{w}^T \\cdot \\mathbf{w} = 1\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "From the first equation we conclude that the optimum vector $\\mathbf{w}$ has been found as an **eigenvector** of the covariance matrix.\n",
    "\n",
    "The second equation just states the fact that $\\mathbf{w}$ has unit length.\n",
    "\n",
    "We have defined \n",
    "\n",
    "$$\n",
    "V = \\mathbf{w}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "inserting the optimum vector yields:\n",
    "\n",
    "$$\n",
    "V(\\mathbf{w}) = \\lambda \\mathbf{w}^T \\cdot \\mathbf{w}= \\lambda ||\\mathbf{w}||^2 = \\lambda\n",
    "$$\n",
    "\n",
    "\n",
    "The MSE is minimised if we choose the eigenvector $\\mathbf{w} = \\mathbf{w}_1$ with the largest eigenvalue $\\lambda = \\lambda_1$.\n",
    "\n",
    "For the residual vector we get:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_{j(1)} = \\mathbf{x}_j - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}_1\\right) \\cdot \\mathbf{w}_1\n",
    "$$\n",
    "\n",
    "and for the average of squared residuals:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{r}_{j(1)}||^2 = \\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{x}_{j}||^2 - \\mathbf{w}_1^T \\cdot \\mathbf{X} \\cdot \\mathbf{w}_1 \\\\\n",
    "\\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{r}_{j(1)}||^2 = \\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{x}_{j}||^2 - \\lambda_1\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5494b5-62ae-4cf7-bacb-d554d318aa89",
   "metadata": {},
   "source": [
    "## Going one step further\n",
    "\n",
    "We are going to project vector $\\mathbf{r}_{j(1)}$ onto some other vector $\\mathbf{w}_2$ and get a new residual vector $\\mathbf{r}_{j(2)}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_{j(2)} = \\mathbf{r}_{j(1)} - \\left(\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 \\right) \\cdot \\mathbf{w}_2 \n",
    "$$\n",
    "\n",
    "Then vector $\\mathbf{w}_2$ shall be chosen such as to minimise the sum of the squared residuals:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{r}_{j(2)}||^2 = \\frac{1}{N} \\sum_{j=1}^N \\mathbf{r}_{j(2)}^T \\cdot \\mathbf{r}_{j(2)} = \\frac{1}{N} \\sum_{j=1}^N \\left( ||\\mathbf{r}_{j(1)}||^2 - \\left(\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 \\right)^2 \\right) = \\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{r}_{j(1)}||^2  - \\frac{1}{N} \\sum_{j=1}^N \\left(\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 \\right)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{r}_{j(2)}||^2 = \\left(\\frac{1}{N} \\sum_{j=1}^N ||\\mathbf{x}_{j}||^2 - \\lambda_1 \\right)   - \\frac{1}{N} \\sum_{j=1}^N \\left(\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 \\right)^2\n",
    "$$\n",
    "\n",
    "Choose $\\mathbf{w}_2$ to maximise \n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{j=1}^N \\left(\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 \\right)^2\n",
    "$$\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 = \\left( \\mathbf{x}_j - \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}_1\\right) \\cdot \\mathbf{w}_1 \\right)^T \\cdot \\mathbf{w}_2 \\\\\n",
    "\\ \\\\\n",
    "\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 = \\mathbf{x}_j^T \\cdot \\mathbf{w}_2 - \\mathbf{w}_1^T \\cdot \\left(\\mathbf{x}_j^T \\cdot \\mathbf{w}_1\\right) \\cdot \\mathbf{w}_2\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "At this point we postulate the vectors $\\mathbf{w}_1$ and $\\mathbf{w}_2$ are **orthonormal**.\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 = \\mathbf{x}_j^T \\cdot \\mathbf{w}_2 \n",
    "$$\n",
    "\n",
    "Hence \n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{j=1}^N \\left(\\mathbf{r}_{j(1)}^T \\cdot \\mathbf{w}_2 \\right)^2 = \\frac{1}{N} \\sum_{j=1}^N \\mathbf{w}_2^T \\left(\\mathbf{x}_j \\mathbf{x}_j^T\\right) \\cdot \\mathbf{w}_2 = \\mathbf{w}_2^T \\cdot \\left( \\frac{1}{N} \\sum_{j=1}^N \\mathbf{x}_j \\mathbf{x}_j^T \\right) \\cdot \\mathbf{w}_2 = \\mathbf{w}_2^T \\cdot \\mathbf{X} \\cdot \\mathbf{w}_2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188f0fa-b1be-4221-b4e2-6bdde063ab56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
