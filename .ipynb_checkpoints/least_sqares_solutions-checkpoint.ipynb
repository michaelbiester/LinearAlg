{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1291ce09-74e0-4e8c-8374-19d46218de44",
   "metadata": {},
   "source": [
    "#  Least Squares Solutions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1345ae-2caf-4642-8655-241b17d9d4a4",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "Let $\\mathbf{A}$ denote a $m \\times n$ matrix. The equation \n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "shall be solved. In general the solution is not unique. Since $\\mathbf{A} \\cdot \\mathbf{x}$ is a weighted addition of the column vectors of matrix $\\mathbf{A}$ vector $\\mathbf{b}$ must be in the column-space if a solution exists. In general only an approximation\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b} + \\mathbf{r}\n",
    "$$\n",
    "\n",
    "can be found. $\\mathbf{r}$ is the residual vector.\n",
    "\n",
    "$$\n",
    "\\mathbf{r} = \\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Because $\\mathbf{r}$ is not in the column space of matrix $\\mathbf{A}$ the vector must be orthogonal to each column of $\\mathbf{A}$. This condition is equivalent to this equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^T \\cdot \\mathbf{r} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{A}^T \\cdot \\left( \\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{b} \\right) = \\mathbf{0} \\\\\n",
    "\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "\\underbrace{\\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\right)^{-1} \\cdot \\mathbf{A}^T}_{left \\ inverse} \\cdot \\mathbf{A} \\cdot \\mathbf{x} = \\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\right)^{-1} \\cdot \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "\\mathbf{x} = \\underbrace{\\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\right)^{-1} \\cdot \\mathbf{A}^T}_{left \\ inverse} \\cdot \\mathbf{b} \n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "So if the `left-inverse` can be computed, the equation  $\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b} + \\mathbf{r}$ can be solved.\n",
    "\n",
    "---\n",
    "\n",
    "A different approach to the solution of equation\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b} + \\mathbf{r}\n",
    "$$\n",
    "\n",
    "is to minimise the L2 norm of the residual vector $||\\mathbf{r}||$. This is equivalent to the minimisation of $||\\mathbf{r}||^2=\\mathbf{r}^T \\cdot \\mathbf{r}$\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{r}^T \\cdot \\mathbf{r} = \\left( \\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{b} \\right)^T \\cdot \\left( \\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{b} \\right) \\\\\n",
    "\\mathbf{r}^T \\cdot \\mathbf{r} = \\left(\\mathbf{x}^T \\cdot \\mathbf{A}^T - \\mathbf{b}^T \\right) \\cdot \\left(\\mathbf{A} \\cdot \\mathbf{x} -\\mathbf{b} \\right) \\\\\n",
    "f(\\mathbf{x}) = \\mathbf{r}^T \\cdot \\mathbf{r} = \\mathbf{x}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} - 2 \\cdot\\mathbf{b}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} +  \\mathbf{b}^T \\cdot \\mathbf{b}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Thus the goal is to minimize a scalar function $f(\\mathbf{x})$:\n",
    "\n",
    "Regardless of the shape of matrix $\\mathbf{A}$ the matrix $\\mathbf{U}$ is square, symmetric and positive definite.\n",
    "\n",
    "The gradient of $f(\\mathbf{x})$ is computed like this:\n",
    "\n",
    "$$\n",
    "f'(\\mathbf{x}) = \\left[\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial x_1} f(\\mathbf{x}) \\\\\n",
    "\\frac{\\partial}{\\partial x_2} f(\\mathbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_N} f(\\mathbf{x})\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial x_1} \\left(\\mathbf{x}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}\\right) \\\\\n",
    "\\frac{\\partial}{\\partial x_2} \\left(\\mathbf{x}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_N} \\left(\\mathbf{x}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}\\right)\n",
    "\\end{array}\n",
    "\\right] -\n",
    "2 \\cdot \\left[ \\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial x_1} \\mathbf{b}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} \\\\\n",
    "\\frac{\\partial}{\\partial x_2} \\mathbf{b}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_N} \\mathbf{b}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\\begin{gather}\n",
    "f'(\\mathbf{x}) = \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} + \\mathbf{A} \\cdot \\mathbf{A}^T \\cdot \\mathbf{x} - 2 \\cdot \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "f'(\\mathbf{x}) = 2 \\cdot \\left( \\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} - \\mathbf{A}^T \\cdot \\mathbf{b}\\right) \n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Setting the gradient to $0$ results in the *normal* equation for the unknown vector $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{A}^T \\cdot \\mathbf{b}\n",
    "$$\n",
    "\n",
    "**Note**\n",
    "\n",
    "By inserting $\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b} + \\mathbf{r}$ we get\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{A}^T \\cdot \\left(\\mathbf{b} + \\mathbf{r} \\right) = \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "\\mathbf{A}^T \\cdot \\mathbf{b} + \\mathbf{A}^T \\cdot \\mathbf{r} = \\mathbf{A}^T \\cdot \\mathbf{b} \\\\\n",
    "\\to \\\\\n",
    "\\mathbf{A}^T \\cdot \\mathbf{r} = \\mathbf{0}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "This equation shows again that the residual vector $\\mathbf{r}$ is orthogonal to all columns of matrix $\\mathbf{A}$.\n",
    "\n",
    "If the inverse $\\left(\\mathbf{A}^T \\cdot \\mathbf{A}\\right)^{-1}$ exist vector $\\mathbf{x}$ is computed by:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\underbrace{\\left(\\mathbf{A}^T \\cdot \\mathbf{A}\\right)^{-1} \\cdot \\mathbf{A}^T}_{left \\ inverse} \\cdot \\mathbf{b}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The solution can be found using `Numpy` method `numpy.linalg.lstsq`\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caca064-6ec3-44af-9a6a-e8f94c932529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396e8bc-fe81-40f0-ac4c-b39ee65ac9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d5b6e-fb39-4f2b-9cac-77c6b1e784f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
